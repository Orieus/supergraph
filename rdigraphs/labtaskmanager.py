import os
# import shutil
# import _pickle as pickle

# import yaml

import numpy as np
import pandas as pd
# from scipy.sparse import csr_matrix
# import colored
import time
import logging

# # import configparser
from collections import Counter
from copy import copy

# Set matplotib parameters to allow remote executions
import matplotlib
matplotlib.use("Agg")

# Imports for the community label extraction
# Maybe this does not need to be after matplotlib.use()
import gensim.corpora.bleicorpus as blei
from sklearn.preprocessing import normalize

# # Local imports
from rdigraphs.sgtaskmanager import SgTaskManager
from rdigraphs.supergraph.supergraph import SuperGraph
from rdigraphs.sim_graph.sim_graph import SimGraph
from rdigraphs.community_plus.community_plus import CommunityPlus

import matplotlib.pyplot as plt


class LabTaskManager(SgTaskManager):
    """
    Task Manager for the RDIgraph analyzer.

    The behavior of this class depends on the state of the project, in
    dictionary self.state, with the followin entries:

    - 'isProject'   : If True, project created. Metadata variables loaded
    - 'configReady' : If True, config file succesfully loaded. Datamanager
                      activated.
    """

    def __init__(self, path2project, paths2data):
        """
        Initializes the LabTaskManager object

        Parameters
        ----------
        path2project : str
            Path to the graph processing project
        paths2data : dict
            Paths to data sources
        """

        super().__init__(path2project, paths2data)

        return

    def _plot_results(self, x, y, xlabel, ylabel, fpath, xscale='log',
                      yscale='log'):
        """
        Generic plot for the data dictionaries generated by the graph
        analysis

        Parameters
        ----------
        x : dict
            A dictionary of lists of arrays of x values
        y : dict
            A dictionary of arrays of y values
        xlabel : dict
            A dictionary of x-labels
        ylabel : dict
            A dictionary of y-labels
        fpath : str
            Path to the output file
        xscale : str {'log', 'lin'}, optional (default='log')
            Scale of the x coordinate (log or lin)
        yscale : str {'log', 'lin'}, optional (default='log')
            Scale of the y coordinate (log or lin)
        """

        plt.figure()

        # A quick one-line to sort the models in a natural order (this has
        # an influence on the order of the legend plot)
        sorted_models = [z[1] for z in sorted([(int(k.split('_')[-1]), k)
                                               for k in x])]
        for model in sorted_models:
            if xscale == 'log' and yscale == 'log':
                plt.loglog(x[model], y[model], '.:', label=model)
            elif xscale == 'log' and yscale == 'lin':
                plt.semilogx(x[model], y[model], '.:', label=model)
            elif xscale == 'lin' and yscale == 'log':
                plt.semilogy(x[model], y[model], '.:', label=model)
            else:
                plt.plot(x[model], y[model], '.:', label=model)

        plt.xlabel(xlabel)
        plt.ylabel(ylabel)
        plt.legend()
        plt.show(block=False)
        plt.savefig(fpath)

        return

    def get_source_info(self):
        """
        Extracts basic information about the doc-topic matrices in the source
        folders.

        For each matrix, plot the distribution of the number of nonzero topics
        """

        folders = os.listdir(self.path2tm)
        p2p = self.path2project   # This is just to abbreviate
        out_dir = os.path.join(p2p, self.f_struct['output'], 'source_info')
        if not os.path.exists(out_dir):
            os.makedirs(out_dir)

        models, n_items, n_topics, n_nz, r_nz = [], [], [], [], []

        # Explore all source folders contatinig a file names modelo_sparse.npz
        for folder in folders:

            # Load data matrix
            path = os.path.join(self.path2tm, folder)
            fpath = os.path.join(path, 'modelo_sparse.npz')

            if os.path.isfile(fpath):
                # Load corpus data
                logging.info(f'---- Loading model {folder}')
                data, df_nodes = self.DM.readCoordsFromFile(
                    path, sparse=True, ref_col='corpusid')
                T = data['thetas']
                # nodes = df_nodes['corpusid'].tolist()

                # Keep metadata
                models.append(folder)
                n_items.append(T.shape[0])
                n_topics.append(T.shape[1])
                n_nz.append(T.count_nonzero())
                r_nz.append(100 * n_nz[-1] / n_items[-1] / n_topics[-1])

                # Plot distribution of the number of nonzero topics
                num_topics = (T > 0).sum(axis=1).T.tolist()[0]
                nt_hist = Counter(num_topics)

                plt.figure()
                plt.stem(nt_hist.keys(), nt_hist.values())
                plt.xlabel('Number of nonzero topics')
                plt.ylabel('Number of documents')
                plt.title(folder)
                plt.show(block=False)
                fpath = os.path.join(out_dir, f'ntopics_hist_{folder}.png')
                plt.savefig(fpath)
            else:
                logging.info(f'---- Model {folder} not available')

        # Create summary table
        df = pd.DataFrame({'Name': models,
                           'Topics': n_topics,
                           'Items': n_items,
                           'Non-zeros': n_nz,
                           'Ratio nonzeros (%)': r_nz})

        # Save summary table
        out_path = os.path.join(p2p, self.f_struct['output'],
                                'source_models.xls')
        df.to_excel(out_path)

        return

    def get_equivalent_classes(self, corpus):
        """
        Extracts basic information about the doc-topic matrices in the source
        folders.

        For each matrix, plot the distribution of the number of nonzero topics

        Parameters
        ----------
        corpus : str
            Name of the corpus
        """

        # Parameters
        fname = 'modelo_sparse.npz'     # Name of topic model files
        out_fname = f'equivalence_classes_{corpus}.xls'

        # ########################
        # Location of data sources
        # ########################

        # Prepare ouput folder
        p2p = self.path2project   # This is just to abbreviate
        out_dir = os.path.join(p2p, self.f_struct['output'], 'source_info',
                               'equiv_classes')
        if not os.path.exists(out_dir):
            os.makedirs(out_dir)

        # Get the parameter of the selected collection of topic models
        if corpus == 'noACL':
            root_dir = self.path2tm
            alg_level = False
            path2nodenames = None
            ref_col = 'corpusid'
        elif corpus == 'ACL':
            root_dir = self.path2ACL
            alg_level = True
            path2nodenames = os.path.join(self.path2ACL, 'corpus', 'ACL',
                                          'allmetadata.csv')
            ref_col = 'ACLid'

        # Get paths to all folders containing a topic model file
        paths = []
        for dir_name, subdirs, files in os.walk(root_dir, topdown=False):
            if fname in files:
                paths.append(dir_name)

        # #################################
        # Computation of equivalent classes
        # #################################

        models, algs, n_nodes, n_topics, sparsity = [], [], [], [], []
        n_1links, n_clusters, n_preclusters = [], [], []

        # Explore all source folders contatinig a file names modelo_sparse.npz
        for path in paths:

            parent, folder = os.path.split(path)
            if alg_level:
                parent, alg = os.path.split(parent)
                model = f'{folder}_{alg}'
            else:
                model = folder
                alg = None

            # Load data matrix
            logging.info(f'-- Equivalent classes for model {model}')

            # Load corpus data
            data, df_nodes = self.DM.readCoordsFromFile(
                path, sparse=True, ref_col=ref_col,
                path2nodenames=path2nodenames)
            T = data['thetas']
            # nodes = df_nodes[ref_col].tolist()

            # Plot distribution of the number of nonzero topics
            logging.info(f'-- -- Computing classes')

            # Compute equivalent classes
            sg = SimGraph(T)
            sg.cluster_equivalent_nodes()

            models.append(model)
            algs.append(alg)
            n_nodes.append(T.shape[0])
            n_topics.append(T.shape[1])
            sparsity.append(1 - T.count_nonzero() / np.prod(T.shape))
            n_1links.append(sg.n_edges)
            n_preclusters.append(sg.n_preclusters)
            n_clusters.append(sg.n_clusters)

            # Show main results
            print(f"-- -- Number of nodes = {n_nodes}")
            print(f"-- -- Clusters = {n_clusters}")
            print(f"-- -- Unit_links = {n_1links}")

        # #################################
        # Computation of equivalent classes
        # #################################

        # Create summary table
        df = pd.DataFrame({'Model': models,
                           'Algorithm': algs,
                           'Nodes': n_nodes,
                           'Topics': n_topics,
                           'Sparsity': sparsity,
                           'Equivalent classes': n_clusters,
                           'Distinct nonzero patterns': n_preclusters,
                           'Unit links': n_1links})
        print(df)

        # Save summary table of equivalence classes
        out_path = os.path.join(out_dir, out_fname)
        df.to_excel(out_path)

        return

    def show_equivalent_classes(self):
        """
        Show equivalent classes
        """

        # ###############
        # Read data files
        # ###############

        # Read the file names in the folder containing the xls reports
        p2p = self.path2project   # This is just to abbreviate
        data_dir = os.path.join(p2p, self.f_struct['output'], 'source_info',
                                'equiv_classes')
        fname = f'equivalence_classes_ACL.xls'

        # Read report from file
        fpath = os.path.join(data_dir, fname)
        df = pd.read_excel(fpath)

        # Select datat o plot
        keys = ['Equivalent classes', 'Unit links', 'Sparsity',
                'Distinct nonzero patterns']
        nfigs = len(keys)

        # Create figure handlers
        fig, ax = [None] * nfigs, [None] * nfigs
        for i in range(nfigs):
            fig[i], ax[i] = plt.subplots()

        # Plot results
        algorithms = sorted(list(set(df['Algorithm'].tolist())))
        for a in algorithms:
            df_a = df[df['Algorithm'] == a].sort_values('Topics')
            nt = df_a['Topics']

            for i, key in enumerate(keys):
                y = df_a[key]
                ax[i].plot(nt, y, '.:', label=a)
                ax[i].set_xlabel('Topics')
                ax[i].set_ylabel(key)

        # Complete and save figures
        for i, key in enumerate(keys):
            ax[i].legend()
            ax[i].grid()
            key_ = key.replace(' ', '_')
            fig[i].savefig(os.path.join(data_dir, f'{key_}.png'))

        plt.show(block=False)

        return

    def analyze_radius(self, corpus):
        """
        This method analyzes the generation of similarity (semantic) graphs.
        More specifically, it explores the variation of several graph
        parameters as a function of the radius, for a fixed number of nodes,
        for a given corpus and for several topic models (from the same corpus)

        The radius is the bound on the JS distance used to sparsify the graph.
        The number of nodes is a subsample of the total number of nodes.

        Parameters
        ----------
        corpus : str
            Corpus. All topic models based on this corpus will be will be
            analyzed.
        """

        # ##########
        # Parameters

        n_gnodes = 5000      # Number of nodes
        Rmax = 1.00001       # Maximum radius of the similarity graph
        sim = 'He2->JS'      # Similarity measure

        # Select all models for the given corpus:
        # The mixed corpus is excluded
        folders = os.listdir(self.path2tm)
        models = [f for f in folders if (f.split('_')[0] == corpus)
                  and (f != 'FECYT_AI_SCOPUS_AI_PATSTAT_AI_120')]

        # Output folder
        p2p = self.path2project   # This is just to abbreviate
        out_dir = os.path.join(p2p, self.f_struct['output'], 'thresholding')
        if not os.path.exists(out_dir):
            os.makedirs(out_dir)

        radius, comp_time, n_edges, density, cc, cmax = {}, {}, {}, {}, {}, {}

        # #######################
        # Model-by-model analysis
        for model in models:

            # Select topic model
            path = os.path.join(self.path2tm, model)

            # #####################
            # Document-topic matrix

            # Load doc-topic matrix for the whole corpus
            logging.info(f'-- Loading data matrix from {path}')
            data, df_nodes = self.DM.readCoordsFromFile(
                path, sparse=True, ref_col='corpusid')
            T = data['thetas']
            nodes = df_nodes['corpusid'].tolist()

            # Take a random sample with n_gnodes
            np.random.seed(3)    # For comparative purposes (provisional)
            idx = np.random.choice(range(len(nodes)), n_gnodes, replace=False)
            T = T[idx]
            nodes = [nodes[i] for i in idx]

            # ########################
            # Compute similarity graph

            comp_time[model], n_edges[model], density[model] = [], [], []
            cc[model], cmax[model] = [], []

            radius[model] = np.linspace(0.1, Rmax, num=19)
            self.SG.makeSuperNode(label=model, nodes=nodes, T=T)

            # Compute similarity graph for each value of the radius.
            for R in radius[model]:
                print(f"-- R = {R}")

                # Create datagraph with the full feature matrix
                t0 = time.time()
                self.SG.computeSimGraph(model, R=R, n_gnodes=n_gnodes,
                                        similarity=sim, verbose=False)
                comp_time[model].append(time.time() - t0)

                # Compute connected components
                self.SG.snodes[model].detectCommunities(alg='cc', label='cc')

                # Get output parameters
                md = self.SG.snodes[model].metadata
                n_edges[model].append(md['edges']['n_edges'])
                density[model].append(100 * md['edges']['density'])
                cc[model].append(md['communities']['cc']['n_communities'])
                cmax[model].append(md['communities']['cc']['largest_comm'])

            # ############
            # Save results

            # Create summary table
            df = pd.DataFrame({'Radius': radius[model],
                               'Time': comp_time[model],
                               'Number of edges': n_edges[model],
                               'Density (%)': density[model],
                               'Connected components': cc[model],
                               'Largest component': cmax[model]})
            print(df)

            # Save summary table
            fname = f'radius_effect_{model}.xls'
            out_path = os.path.join(out_dir, fname)
            df.to_excel(out_path)

        # Plot results
        xlabel = 'Radius'

        fpath = os.path.join(out_dir, f'{corpus}_comp_time.png')
        self._plot_results(
            radius, comp_time, xlabel, 'Time', fpath, xscale='lin')

        fpath = os.path.join(out_dir, f'{corpus}_density.png')
        self._plot_results(
            radius, density, xlabel, 'Density', fpath, xscale='lin')

        fpath = os.path.join(out_dir, f'{corpus}_n_edges.png')
        self._plot_results(
            radius, n_edges, xlabel, 'Number of edges', fpath, xscale='lin')

        fpath = os.path.join(out_dir, f'{corpus}_CC.png')
        self._plot_results(
            radius, cc, xlabel, 'Number of connected components', fpath,
            xscale='lin')

        fpath = os.path.join(out_dir, f'{corpus}_maxCC.png')
        self._plot_results(
            radius, cmax, xlabel, 'Largest connected component', fpath,
            xscale='lin')

        return

    def _analyze_sampling(self, corpus, n_links=None, max_nodes=None, tag=""):
        """
        This method analyzes the effect of sampling for a fixed number of
        edges.

        Several graph parameters are computed for the given corpus, for
        different topic models and for a given number of target links, as a
        function of the sampling factor (i.e. the number of nodes)

        It uses the SimGraph class to compute the graphs. Since the number of
        edges cannot be fixed (only the number of nodes and the radius can
        be can be set in advance) we proceed in two steps: in a first step, we
        compute a denser graph, with a large number of nodes, and, based on it,
        we compute the Radius required to get the desired number of edges.

        UPDATE: The current version of the computeSimGraph() method from class
        simgraph admits entering the number of edges. The code below could be
        simplifying taking advantage of this new feature.

        Parameters
        ----------
        corpus : str
            Corpus. All topic models based on this corpus will be analyzed
        n_links : int
            Target number of links. If None, the output graph should be fully
            connected
        max_nodes : int
            Maximum number of nodes. If the corpus has more nodes, it is
            subsampled.
        tag : str
            Label identifier for the output file names.
        """

        # ##########
        # Parameters
        sim = 'He2->JS'       # Similarity measure
        fully_connected = n_links is None

        # Select all models for the given corpus:
        folders = os.listdir(self.path2tm)
        models = [f for f in folders if (f.split('_')[0] == corpus)
                  and (f != 'FECYT_AI_SCOPUS_AI_PATSTAT_AI_120')]

        comp_time, n_edges, cc, cmax, rmax, n_nodes = {}, {}, {}, {}, {}, {}
        c_rel = {}

        # #######################
        # Model-by-model analysis
        for model in models:

            # Select topic model
            path = os.path.join(self.path2tm, model)
            p2p = self.path2project   # This is just to abbreviate

            # #####################
            # Document-topic matrix

            # Load doc-topic matrix for the whole corpus
            logging.info(f'-- Loading data matrix from {path}')
            data, df_nodes = self.DM.readCoordsFromFile(
                path, sparse=True, ref_col='corpusid')
            T = data['thetas']
            nodes = df_nodes['corpusid'].tolist()

            if max_nodes is not None:
                max_nodes = min(max_nodes, T.shape[0])
                # Data sampling
                np.random.seed(3)    # For comparative purposes (provisional)
                # Randomize node ordering
                idx = np.random.choice(
                    range(max_nodes), max_nodes, replace=False)
                T = T[idx]
                nodes = [nodes[i] for i in idx]
            n_tot = len(nodes)

            # Select the range of sampling factors
            # spf = [0.2, 0.4, 0.6, 0.8, 1.0]
            spf = np.logspace(-9, 0, num=20, base=2)
            comp_time[model], n_edges[model], n_nodes[model] = [], [], []
            cc[model], cmax[model], rmax[model], c_rel[model] = [], [], [], []
            radius = 1

            for r in spf:

                # Select the reduced data matrix
                n_gnodes = int(r * n_tot)
                # Compute minimal sampling factor.
                if fully_connected:
                    n_links = n_gnodes * (n_gnodes - 1) // 2

                if n_links <= n_gnodes * (n_gnodes - 1) // 2:
                    # We arrive here only if the number of nodes is enough to
                    # get n_links
                    Tr = T[:n_gnodes]
                    nodes_r = nodes[:n_gnodes]
                    print(
                        f"-- -- Sampling up to {n_gnodes} and {n_links} links")

                    # First Trial: since we do not know the value of R required
                    # to get n_links, we compute a denser network first:
                    self.SG.makeSuperNode(label=model, nodes=nodes_r, T=Tr)
                    t0 = time.time()
                    self.SG.computeSimGraph(
                        model, R=radius, n_gnodes=n_gnodes, similarity=sim,
                        verbose=False)
                    print(f"-- -- Tentative simgraph in {time.time()-t0} secs")

                    # Sort weights and get the weight ranked n_links...
                    lw = len(self.SG.snodes[model].weights)
                    w = sorted(
                        list(zip(self.SG.snodes[model].weights, range(lw))),
                        reverse=True)
                    w = w[:n_links]
                    w_min = w[-1][0]

                    # Compute the threshold required to get the target links
                    # This is the radius that should provide n_links edges.
                    radius = np.sqrt(1 - w_min) * radius
                    if radius == 0:
                        logging.warning("-- -- All selected links, weight 1")
                    radius = max(1e-20, radius)

                    # Compute graph with the target number of links
                    t0 = time.time()
                    self.SG.computeSimGraph(model, R=radius, n_gnodes=n_gnodes,
                                            similarity=sim, verbose=False)
                    dt = time.time() - t0
                    comp_time[model].append(dt)
                    print(f"-- -- Simgraph in {dt} seconds")

                    # Compute connected components
                    self.SG.snodes[model].detectCommunities(
                        alg='cc', label='cc')

                    # Get output parameters
                    md = self.SG.snodes[model].metadata
                    rmax[model].append(radius)
                    n_nodes[model].append(n_gnodes)
                    n_edges[model].append(md['edges']['n_edges'])
                    cc[model].append(md['communities']['cc']['n_communities'])
                    cmax[model].append(md['communities']['cc']['largest_comm'])
                    c_rel[model].append(
                        md['communities']['cc']['largest_comm'] / n_gnodes)

                    # Remove snode, because it is no longer required
                    self.SG.drop_snode(model)

            # ############
            # Save results

            # Create summary table
            df = pd.DataFrame({'Radius': rmax[model],
                               'Time': comp_time[model],
                               'Number of nodes': n_nodes[model],
                               'Number of edges': n_edges[model],
                               'Connected components': cc[model],
                               'Largest component': cmax[model],
                               'Relative max CC': c_rel[model]})
            print(df)

            # Save summary table
            fname = f'{tag}_{model}.xls'
            out_dir = os.path.join(p2p, self.f_struct['output'], 'sampling')
            if not os.path.exists(out_dir):
                os.makedirs(out_dir)
            out_path = os.path.join(out_dir, fname)
            df.to_excel(out_path)

        # Plot results
        xlabel = 'Number of nodes'
        fpath = os.path.join(out_dir, f'{tag}_{corpus}_comp_time.png')
        self._plot_results(n_nodes, comp_time, xlabel, 'Time', fpath)

        fpath = os.path.join(out_dir, f'{tag}_{corpus}_CC.png')
        self._plot_results(
            n_nodes, cc, xlabel, 'Number of connected components', fpath)

        fpath = os.path.join(out_dir, f'{tag}_{corpus}_maxCC.png')
        self._plot_results(
            n_nodes, cmax, xlabel, 'Largest connected component', fpath)

        fpath = os.path.join(out_dir, f'{tag}_{corpus}_relCC.png')
        self._plot_results(
            n_nodes, c_rel, xlabel, 'Relative size of the largest CC', fpath)

        fpath = os.path.join(out_dir, f'{tag}_{corpus}_rmax.png')
        self._plot_results(
            n_nodes, rmax, xlabel, 'Radius', fpath, yscale='lin')

        return

    def analyze_sampling(self, corpus):
        """
        Analyze sampling for a given corpus

        Parameters
        ----------
        corpus : str
            Corpus. All topic models based on this corpus will be will be
            analyzed.
        """

        self._analyze_sampling(corpus, n_links=400_000, tag='sampling',
                               max_nodes=200_000)

        return

    def _validate_topic_models(self, path2models, d, spf, rescale=True,
                               radius=None, n_edges_t=None, g=1,
                               cd_algs=[], metrics=[]):
        """
        Analyzes the influence of the topic model and the similarity graph
        parameters on the quality of the graph.

        The similarity graph is validated using a citations graph.

        Parameters
        ----------
        path2models : str
            Path specifying the class of models to validate.
        d : float
            Distance measure (JS, He or l1)
        spf : float
            Sampling factor. To reduce the corpus size.
        rescale : bool, optional (default=True)
            If True, it rescales similarity weights to span from 0 to 1.
        radius : float or None, optional (default=None)
            Distance threshold for the sparse graph generation.
            If None then an interger n_edges_t must be specified
        n_edges_t : int or None, optional (default=None)
            Target number of edges of the sparse graph.
            If None then a value of radius must be specified
        g : float, optional (default=1)
            Exponent for the conversion distances-weights
        cd_algs : list of str, optional (default=[])
            Community detection algorithms to validate
        metrics : list of str, optional (default=[])
            Metrics to evaluate for community comparison.

        Notes
        -----
            radius and n_edges_t are mutually exclusive parameters: one and
            only one must be specified.
        """

        # ################
        # Corpus selection

        # Corpus and model specification
        corpus = 'ACL'
        # Name of the column in metadata files containing the node identifiers
        ref_col = 'ACLid'

        # ##########
        # Parameters

        if radius is None and n_edges_t is None:
            logging.error("-- -- One of radius or n_edges_t parameters must "
                          "be specified")
        elif radius is not None and n_edges_t is not None:
            logging.error("-- -- Only one radius or n_edges_t parameters "
                          "must be specified")

        # Read the type of topic models from the path name.
        tm_class = os.path.split(path2models)[-1]

        # Take the similarity measure corresponding to the selected distance
        smap = {'JS': 'He2->JS', 'He': 'He2', 'l1': 'l1'}
        sim = smap[d]

        # #####################
        # Paths to topic models

        # Path to the folder containing the node names (same for all)
        path2nodenames = os.path.join(
            self.path2ACL, 'corpus/ACL/allmetadata.csv')
        # Path to the folder containing the folders with the topic models
        folders = os.listdir(path2models)

        # #######################
        # Load co-citations graph

        # Load co-citations graph from DB or from a file in the supergraph
        logging.info('-- -- Loading scitations graph...')
        label_cocit = f'cocit_spf{int(1000*spf)}'

        if label_cocit in self.SG.metagraph.nodes:
            # If the selected cocitation subgraph already exists, just load it
            # from file.
            self.SG.activate_snode(label_cocit)
            nodes_ref = self.SG.snodes[label_cocit].nodes
            n_gnodes = len(nodes_ref)

            # List of indices of the selected nodes. This will be used to
            # select the topic vectors in the topic matrix
            np.random.seed(3)
            idx = sorted(
                np.random.choice(range(n_gnodes), n_gnodes, replace=False))
        else:
            if 'cocitations' not in self.SG.metagraph.nodes:
                # Load co-citations graph from database.
                self.import_co_citations_graph()
            # Activate cocitations graph
            # (This is necessary no matter if self.import_co_citations_graph()
            # was invoked, because self.import_co_citations_graph deactivates
            # the graph, and we need re-activation to reload from file)
            self.SG.activate_snode('cocitations')

            # Read list of nodes.
            # The nodes in the citations graph and in the topic models are not
            # exactly the same. We will construct a version of the citation
            # graph with the nodes selected from the topic models.

            # Read list of nodes from the topic models
            df_nodes_tm = pd.read_csv(path2nodenames, usecols=[ref_col])
            n_tm = len(df_nodes_tm)
            # Sub-sample the list.
            # This is done because the complete list of nodes may be too large
            # to build dense or almost dense graphs
            n_gnodes = int(spf * n_tm)   # Target number of nodes
            np.random.seed(3)
            idx = sorted(
                np.random.choice(range(n_gnodes), n_gnodes, replace=False))
            nodes_ref = df_nodes_tm.iloc[idx][ref_col].tolist()

            # Subsample the cocitations graph
            self.SG.sub_snode('cocitations', nodes_ref, ylabel=label_cocit)
            self.SG.deactivate_snode('cocitations')

        # df_edges_cg = self.SG.snodes[label_cocit].df_edges

        # Compute communities for the scitation graph
        for alg in cd_algs:
            self.SG.snodes[label_cocit].detectCommunities(alg=alg, label=alg)
        self.SG.save_supergraph()

        # #######################
        # Model-by-model analysis

        # Models to analyze
        models = [f for f in folders if f.split('_')[0] == corpus]

        # Initialize output variables
        n_topics, n_nodes, n_edges, comp_time = [], [], [], []
        cc, cmax, rmax, c_rel, cocit_sim = [], [], [], [], []
        cd_metric = {}
        for alg in cd_algs:
            cd_metric[alg] = {'nc': [], 'lc': [], 'lc_rel': []}
            for m in metrics:
                cd_metric[alg][m] = []
        extended_metrics = metrics + ['nc', 'lc', 'lc_rel']

        for i, model in enumerate(models):

            logging.info(f"-- Model {i} ({model}) out of {len(models)}")

            # Select topic model
            path = os.path.join(path2models, model)

            # #####################
            # Document-topic matrix

            # Load doc-topic matrix for the whole corpus
            data, df_nodes = self.DM.readCoordsFromFile(
                fpath=path, sparse=True, path2nodenames=path2nodenames,
                ref_col=ref_col)
            T = data['thetas']                     # Doc-topic matrix
            # nodes = df_nodes[ref_col].tolist()     # List of node ids

            # ###########
            # Subsampling

            # Note that idx is the same for all models.
            T = T[idx]

            # Compute similarity graph
            self.SG.makeSuperNode(label=model, nodes=nodes_ref, T=T)
            t0 = time.time()
            self.SG.computeSimGraph(
                model, R=radius, n_edges=n_edges_t, n_gnodes=n_gnodes,
                similarity=sim, g=g, rescale=rescale, verbose=False)
            dt = time.time() - t0

            # Compute connected components
            self.SG.snodes[model].detectCommunities(alg='cc', label='cc')

            # Get output parameters
            md = self.SG.snodes[model].metadata
            n_topics.append(T.shape[1])
            n_nodes.append(n_gnodes)
            n_edges.append(md['edges']['n_edges'])
            rmax.append(md['edges']['R'])
            cc.append(md['communities']['cc']['n_communities'])
            cmax.append(md['communities']['cc']['largest_comm'])
            c_rel.append(md['communities']['cc']['largest_comm'] / n_gnodes)
            comp_time.append(dt)

            # Compute similarity with the citations graph
            score = self.SG.cosine_sim(label_cocit, model)
            cocit_sim.append(score)

            # Compute communities for the semantic graphs
            for alg in cd_algs:
                # Compute community
                self.SG.snodes[model].detectCommunities(alg=alg, label=alg)

                md = self.SG.snodes[model].metadata
                cd_metric[alg]['nc'].append(
                    md['communities'][alg]['n_communities'])
                cd_metric[alg]['lc'].append(
                    md['communities'][alg]['largest_comm'])
                cd_metric[alg]['lc_rel'].append(
                    md['communities'][alg]['largest_comm'] / n_gnodes)

                # Compare communities with those of the scitations graph
                comm1 = self.SG.snodes[label_cocit].df_nodes[alg].tolist()
                comm2 = self.SG.snodes[model].df_nodes[alg].tolist()
                edges = self.SG.snodes[label_cocit].edge_ids
                weights = self.SG.snodes[label_cocit].weights
                CD = CommunityPlus()
                for m in metrics:
                    if m in ['modularity', 'coverage', 'performance']:
                        # Community vs graph metric
                        score = CD.community_metric(edges, weights, comm2, m)
                    else:
                        # Community vs community metric
                        score = CD.compare_communities(comm1, comm2, method=m)

                    cd_metric[alg][m].append(score)

            # Remove snode, because it is no longer required
            self.SG.drop_snode(model)

        # ############
        # Save results

        # Sort result variables by number of topics
        # We need to save the original ordeiring of the number of topics to
        # sort the cd metrics afterwards.
        n_topics_0 = copy(n_topics)
        (n_topics, models, n_nodes, n_edges, comp_time, cc, cmax, rmax, c_rel,
            comp_time, cocit_sim) = tuple(zip(*sorted(zip(
                n_topics, models, n_nodes, n_edges, comp_time, cc, cmax, rmax,
                c_rel, comp_time, cocit_sim))))

        # Compute communities for the scitation graph
        for alg in cd_algs:
            for m in metrics:
                (aux, cd_metric[alg][m]) = tuple(zip(*sorted(zip(
                    n_topics_0, cd_metric[alg][m]))))

        # Create summary table
        df = pd.DataFrame({'Model': models,
                           'No. of topics': n_topics,
                           'Radius': rmax,
                           'Time': comp_time,
                           'Number of nodes': n_nodes,
                           'Number of edges': n_edges,
                           'Connected components': cc,
                           'Largest component': cmax,
                           'Relative max CC': c_rel,
                           'Cocitations similarity': cocit_sim})

        # Add community metrics
        for alg in cd_algs:
            for m in extended_metrics:
                col = f"{alg}_{m}"
                df[col] = cd_metric[alg][m]

        print("Summary of results:")
        print(df)

        # Save summary table
        if rescale:
            preffix = f'{corpus}_{sim}_R_{n_gnodes}_{n_edges_t}_ig{int(1/g)}'
        else:
            preffix = f'{corpus}_{sim}_U_{n_gnodes}_{n_edges_t}_ig{int(1/g)}'

        fname = f'{preffix}_{tm_class}.xls'
        out_dir = os.path.join(self.path2project, self.f_struct['output'],
                               'validation', preffix, 'results')
        if not os.path.exists(out_dir):
            os.makedirs(out_dir)
        out_path = os.path.join(out_dir, fname)
        df.to_excel(out_path)

        return

    def validate_topic_models(self, path2models):
        """
        Calls to self._validate_topic_models for a specific value of the radius
        and the sampling factor.

        Parameters
        ----------
        path2models : str
            Path specifying the class of models to validate.
        """

        # ##########
        # Parameters
        d = 'JS'   # Similarity measure
        radius = 1.0001   # Radius threshold for the graph generation
        spf = 0.3         # Sampling factor
        rescale = True

        self._validate_topic_models(path2models, d, spf, radius=radius,
                                    rescale=rescale)

        return

    def validate_all_models_cd(self, d):
        """
        Analyzes the influence of the topic model and the similarity graph
        parameters on the quality of the community structures

        The similarity graph is validated using a citations graph.

        Parameters
        ----------
        d : str
            Similarity measure: options are: JS, He(llinger) or l1.
        """

        # Parameters
        # cd_algs = ['louvain', 'fastgreedy', 'walktrap', 'infomap',
        #            'labelprop']
        cd_algs = ['louvain', 'walktrap', 'labelprop', 'fastgreedy']
        metrics = ['vi', 'nmi', 'rand', 'adjusted_rand', 'split-join',
                   'modularity', 'coverage', 'performance']

        p = self.global_parameters['validate_all_models']
        spf = p['spf']              # Sampling factor
        rescale = p['rescale']      # True to map similarities to [0, 1]
        n_edges_t = p['n_edges_t']  # Target number of edges
        g = p['g']                  # Exponent of the dist-sim transformation
        print(f"Number of edges: {n_edges_t}")

        # Data folder
        folder = os.path.join(self.path2ACL, 'models', 'tm')
        tm_classes = [x for x in os.listdir(folder)
                      if os.path.isdir(os.path.join(folder, x))]

        # Validate modesl, one by one..
        for tmc in tm_classes:
            logging.info(
                f'-- Validating models of type {tmc} with {d} similarity')

            path2models = os.path.join(folder, tmc)
            out_dir = self._validate_topic_models(
                path2models, d, spf, rescale=rescale, n_edges_t=n_edges_t,
                g=g, cd_algs=cd_algs, metrics=metrics)

        return

    def show_validation_results_e(self, path):
        """
        Shows the results of the topic model validation in
        self.validate_topic_models()

        Parameters
        ----------
        path: str
            Path to data
        """

        # ###############
        # Read data files
        # ###############

        # Read the file names in the folder containing the xls reports
        data_dir = os.path.join(path, 'results')
        data_files = sorted(os.listdir(data_dir))

        # Read all result files
        sim, rescale, n_nodes, n_edges, ig, tm_class = [], [], [], [], [], []
        params, df_dict = {}, {}
        # fname_struct = ['corpus', 'sim', 'rescale', 'n_nodes', 'n_edges',
        #                 'ig', 'tm_class']
        for f in data_files:
            if f.endswith('.xls'):
                fname = os.path.splitext(f)[0]
                fname_parts = fname.split('_')

                # Read parameters from the file name
                sim_f = fname_parts[1]
                rescale_f = fname_parts[2]
                n_nodes_f = fname_parts[3]
                n_edges_f = fname_parts[4]
                ig_f = fname_parts[5]
                tm_class_f = '_'.join(fname_parts[6:])

                sim.append(sim_f)
                rescale.append(rescale_f)
                n_nodes.append(n_nodes_f)
                n_edges.append(n_edges_f)
                ig.append(ig_f)
                tm_class.append(tm_class_f)

                # Read report from file
                fpath = os.path.join(data_dir, f)
                df_dict[fname] = pd.read_excel(fpath)
                params[fname] = {
                    'tmc': tm_class_f, 'n': n_nodes_f, 'e': n_edges_f,
                    's': sim_f, 'r': rescale_f, 'i': ig_f}

        # ############
        # Plot results
        # ############

        # Ordered list of parameters.
        sim = sorted(list(set(sim)))
        rescale = sorted(list(set(rescale)))
        n_nodes = sorted(list(set(n_nodes)))
        n_edges = sorted(list(set(n_edges)))
        ig = sorted(list(set(ig)))
        tm_class = sorted(list(set(tm_class)))

        # Get the list of all possiblo dataframe columns to visualize
        cols = []
        for name, df in df_dict.items():
            cols += df.columns.tolist()

        # Remove columns that will not become y-coordinates
        cols = list(set(cols) - {'Unnamed: 0', 'Model', 'No. of topics',
                                 'Number of nodes'})

        # Dictionary of abreviations (for the file names)
        abbreviation = {x: x for x in cols}
        abbreviation.update({'Radius': 'Rad',
                             'Time': 't',
                             'Number of edges': 'ne',
                             'Connected components': 'cc',
                             'Largest component': 'lc',
                             'Relative max CC': 'rc',
                             'Cocitations similarity': 'cs'})

        # The following nested loop is aimed to make multiple plots from xls
        # files inside the same directory. It is actually not needed if all
        # xls files in the given folder have the same parameter values.
        for n in n_nodes:
            for e in n_edges:
                for s in sim:
                    for r in rescale:
                        for i in ig:

                            fnames = [x for x in df_dict if
                                      params[x]['n'] == n
                                      and params[x]['e'] == e
                                      and params[x]['s'] == s
                                      and params[x]['r'] == r
                                      and params[x]['i'] == i]

                            if len(fnames) > 0:

                                for var, ab in abbreviation.items():

                                    fig, ax = plt.subplots()
                                    print(fnames)
                                    # Plot all models with r and n:
                                    for fname in fnames:

                                        tmc = params[fname]['tmc']
                                        x = df_dict[fname]['No. of topics']
                                        y = df_dict[fname][var]
                                        ax.plot(x, y, '.:', label=tmc)

                                    ax.set_xlabel('No. of topics')
                                    ax.set_ylabel(var)
                                    ax.set_title(
                                        f'Nodes: {n}, Edges: {e}, Sim: {s}, '
                                        f'Params: {r + i[-1]}')
                                    ax.legend()
                                    ax.grid()
                                    plt.show(block=False)

                                    # Save figure
                                    out_dir = os.path.join(path, 'figs')
                                    tag = '_'.join(fname_parts[0:5])
                                    fname = f'{tag}_{ab}.png'
                                    if not os.path.exists(out_dir):
                                        os.makedirs(out_dir)
                                    out_path = os.path.join(out_dir, fname)
                                    plt.savefig(out_path)

        return

    def compute_citation_centralities(self, path, n=200):
        """
        Computes all centralities for a given graph, and save in output file
        the top n nodes by centrality

        Parameters
        ----------
        path : str
            Path to snode
        """

        # Create graph object
        logging.info(f'-- Loading GRAPH from {path}')
        label = path.split(os.path.sep)[-1]

        metrics = ['degree', 'centrality', 'betweenness', 'cluster_coef',
                   'pageRank']

        for parameter in metrics:
            # Local graph analysis
            self.SG.local_snode_analysis(label, parameter=parameter)

            # ref_name = self.SG.snodes[label].REF
            df = self.SG.snodes[label].df_nodes.sort_values(
                parameter, axis=0, ascending=False)

            print(f"-- -- Top ranked nodes in {label} by {parameter}")
            print(df.head(10))

            # Save the top n rows
            path2old = os.path.join(self.path2project, self.f_struct['output'],
                                    f'topNaN_{label}_{parameter}.xlsx')
            df.iloc[:n].to_excel(path2old, index=False, encoding='utf-8')
            print(f"-- -- Top {n} saved in {path2old}")

            # Save the top n
            path2out = os.path.join(self.path2project, self.f_struct['output'],
                                    f'top_{label}_{parameter}.xlsx')

            # Remove rows with NaNs. This is aimed for citation graphs
            if 'eid' in df.columns:
                df = df[~pd.isna(df['eid'])]
            df.iloc[:n].to_excel(path2out, index=False, encoding='utf-8')
            print(f"-- -- Top {n} saved in {path2out}")

        # Save
        self.SG.save_supergraph()
        self.SG.deactivate()

        return

    def show_all_citation_centralities(self, path, n=400):
        """
        Shows all available local parameters for a given graph, and save them
        in output files containing the top n nodes for each parameter

        Parameters
        ----------
        path : str
            Path to snode
        """

        # Activate selected snode
        logging.info(f'-- Loading GRAPH from {path}')
        label = path.split(os.path.sep)[-1]
        self.SG.activate_snode(label)
        # ref_name = self.SG.snodes[label].REF

        metrics = ['degree', 'centrality', 'betweenness', 'cluster_coef',
                   'pageRank', 'katz']

        for parameter in metrics:

            if parameter in self.SG.snodes[label].df_nodes.columns:

                logging.info(
                    f"-- -- Computing top nodes in {label} by {parameter}")
                # Sort nodes by parameter value
                df = self.SG.snodes[label].df_nodes.sort_values(
                    parameter, axis=0, ascending=False)

                # Remove rows with NaNs. This is aimed for citation graphs
                if 'eid' in df.columns:
                    df = df[~pd.isna(df['eid'])]

                if parameter == 'cluster_coef':
                    if ('abs_in_degree' in df.columns and 'abs_out_degree' in
                            df.columns):
                        # Filter nodes (only for cluster_coef)
                        logging.info(f"-- -- Filtering nodes with few links")
                        df = df[df['abs_in_degree']
                                + df['abs_out_degree'] >= 10]
                    else:
                        logging.info(
                            f"No in- and out- degree columns available to "
                            "filter nodes with few links")

                df.rename(columns={"title": "Título", "pub_year": "Año",
                                   "citation_count": "Citas"}, inplace=True)

                print(df.head(10))

                # Save the top n
                path2out = os.path.join(
                    self.path2project, self.f_struct['output'],
                    f'top_{label}_{parameter}.xlsx')

                df.iloc[:n].to_excel(path2out, index=True, encoding='utf-8')
                print(f"-- -- Top {n} saved in {path2out}")

        # Save
        self.SG.save_supergraph()
        self.SG.deactivate()

        return

    def read_vocabulary(self, vocab_filename):
        """
        Reads a vocabultary from file.

        Parameters
        ----------
        vocab_filename : str
            Path to vocabulary

        Returns
        -------
        vocab_w2id : dict
            Vocabulary dictionary using words as keys {word_i : id_word_i}
        vocab_id2w : dict
            Vocabulary dictionary using word ids as keys {i : word_i}
        """
        vocab_w2id = {}
        vocab_id2w = {}
        with open(vocab_filename, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                vocab_w2id[line.strip()] = i
                vocab_id2w[str(i)] = line.strip()

        return vocab_w2id, vocab_id2w

    def get_keyword_descriptions(self, vocabfile, W, n_keywords=10):
        """
        Returns a description o each row of a weight matrix. Each column of
        the weight matrix is assumed to contain the weights associated to
        a single word specified in a given vocabulary

        Parameters
        ----------
        vocabfile : str
            Path to vocabulary
        W : numpy array (n_words x n)
            Weight matrix
        n_keywords : int, optional (default=10)
            Number of keywords used to describe each row of the weight matrix
        """

        # Read vocabulary
        vocab_w2id, vocab_id2w = self.read_vocabulary(vocabfile)

        # Select keywords
        tpc = range(W.shape[0])
        tpc_descs = []
        for i in tpc:
            # Compute keywords for the i-th row
            words = [vocab_id2w[str(idx2)]
                     for idx2 in np.argsort(W[i])[::-1][0:n_keywords]]
            tpc_descs.append((i, ', '.join(words)))

        return tpc_descs

    def _get_community_labels(self, path2tm, p2c, n_keywords=10):
        """
        Get a list of keywords characterizing the communities

        Parameters
        ----------
        path2tm : str
            Path to the folder containing the corpus informacions
        p2c : dict
            Dictionary project : communities. Each project is identified by its
            reference
        n_keywords : int, optional (default=10)
            Number of keywords used to describe each community

        Returns
        -------
        W : numpy array
            A weight matrix with one row per community and one column per word
            in the vocabulary

        com_keywords : dict
            A dictionary {community : keywords}
        """

        # Configurable parameters:
        ref_col = 'corpusid'

        # Load vocabulary
        corpus = blei.BleiCorpus(
            os.path.join(path2tm, 'PlanEst_AIall-mult.dat'),
            os.path.join(path2tm, 'PlanEst_AI_vocabulary.gensim'))

        # Initialize word_count matrix, containing the number of repetitions of
        # each word in each community.
        ncoms = max(p2c.values()) + 1            # Number of communities
        vocab_size = len(corpus.id2word)         # Vocabulary size
        word_count = np.zeros((ncoms, vocab_size))

        # Map project references to indices
        path2nodenames = os.path.join(path2tm, 'docs_metadata.csv')
        df_allnodes = pd.read_csv(path2nodenames, usecols=[ref_col])

        # This is not much efficient because I should run over the docs in p2c
        # which are only a fraction of the docs in corpus. However, I do not
        # known how to get random access to docs in corpus.
        for doc_id, bow in enumerate(corpus):
            # Get the reference name associated to doc_id
            ref = df_allnodes.loc[doc_id, ref_col]
            if ref in p2c:
                comm_id = int(p2c[ref])
                for word in bow:
                    word_count[comm_id, word[0]] += word[1]

        # Convert word counts to a weight matrix with probabilistic rows
        W = normalize(word_count, axis=1, norm='l1')

        # Downscalign very common terms usually provides better subcorpus
        # descriptions
        if np.min(W) < 1e-12:
            W += 1e-12
        denom = np.reshape((sum(np.log(W)) / ncoms), (vocab_size, 1))
        denom = np.ones((ncoms, 1)).dot(denom.T)
        W = W * (np.log(W) - denom)

        com_keywords = self.get_keyword_descriptions(
            os.path.join(path2tm, 'vocab.txt'), W, n_keywords=n_keywords)
        com_keywords = dict(com_keywords)

        return W, com_keywords

    def visualize_bigraph(self):
        """
        Generate a bigraph visualization for a demo
        """

        # Parameters
        n0 = 0    # Number of nodes (0 to take all)
        n_epn = 20     # Number of edges per node
        topic_model = 'PlanEst_AI_50'
        # topic_model = 'PEST_120'
        npmin = 50       # Minimum number of projects
        n_keywords = 8   # Nuber of keywords to describe each community

        gP_label = "Proyectos"
        gO_label = "Organismos"    # f"{att}_{gP_label}"
        gC_label = "Comunidades temáticas"   # f"{att}_{gP_label}"
        bgPO_label = f"{gP_label}->{gO_label}"
        bgPC_label = "Proyectos->Comunidades"  # f"{gP_label}_2_{gC_label}"
        bgOC_label = "Organismos->Comunidades"  # f"{gO_label}_2_{gC_label}"
        cd_algorithm = 'leiden'

        # Keep snodes and sedges in memory, because the supergraph is not large
        keep_active_old = self.keep_active  # Preserve old value
        SG_keep_active_old = self.SG.keep_active  # Preserve old value
        self.keep_active = True
        self.SG.keep_active = True

        # Paths to data folders
        path2topicmodels = self.path2tm
        path2graphs = os.path.join(self.path2project, 'graphs')
        path2bigraphs = os.path.join(self.path2project, 'bigraphs')

        # #################
        # Graph of Projects

        # Select topic model
        logging.info('========== 1. Computing graph of PROJECTS')

        # Compute similarity graph
        path2tm = os.path.join(path2topicmodels, topic_model)
        self.infer_sim_graph(path2tm, 'He2', n0=n0, n_epn=n_epn,
                             label=gP_label)
        gP_path = os.path.join(path2graphs, gP_label)

        self.SG.remove_isolated_nodes(gP_label)

        # Add node attributes
        dbname = 'Pr'
        table = 'proyectos'
        atts = ['CIFEMPRESA', 'STATUS', 'CONVOCATORIA']
        att_ref = 'REFERENCIA'
        att_O = 'CIFEMPRESA'
        self.import_node_atts(gP_path, dbname, table, atts, att_ref)

        self.SG.sub_snode_by_novalue(gP_label, 'CIFEMPRESA', None)
        self.SG.sub_snode_by_value(gP_label, 'STATUS', 'Aprobado')
        convs = sorted(list(set(
            self.SG.snodes['Proyectos'].df_nodes.CONVOCATORIA.tolist())))
        print(f"-- -- Proyectos de convocatorias desde {min(convs)} hasta "
              f"{max(convs)}")

        # ############################
        # Graph Projects-Organizations

        # Tranduce graph of entities
        logging.info('========== 2. Computing bigraph PROJECTS->ORGANIZATIONS')
        gO_path = os.path.join(path2graphs, gO_label)
        self.inferBGfromA(gP_path, att_O, t_label=gO_label, e_label=bgPO_label)

        # ####################################
        # Reprocessing to filter organizations
        self.SG.sub_snode_by_threshold(gO_label, 'NSize', npmin, bound='lower')
        selected_Orgs = self.SG.snodes[gO_label].nodes
        self.SG.sub_snode_by_value(gP_label, att_O, selected_Orgs)
        self.inferBGfromA(gP_path, att_O, t_label=gO_label, e_label=bgPO_label)
        # End of reprocessing
        # ####################################

        # Add node attributes
        table = 'organismos'
        atts = ['ORGANISMO OBJ SOCIAL', 'SECTOR', 'TIPO ENTIDAD']
        att_ref = 'CIF'
        self.import_node_atts(gO_path, dbname, table, atts, att_ref)

        # ###########################
        # Graph Projects->Communities

        logging.info('========== 3. Computing bigraph PROJECTS->COMMUNITIES')
        # Communities of projects
        comm_label = "Comunidad"
        self.detectCommunities(cd_algorithm, gP_path, comm_label=comm_label)

        # Transduce graph of communities
        att = comm_label
        self.inferBGfromA(gP_path, att, t_label=gC_label, e_label=bgPC_label)
        bgPC_path = os.path.join(path2bigraphs, bgPC_label)
        self.transduce(bgPC_path, 1)
        gC_path = os.path.join(path2graphs, gC_label)
        comm_label2 = "Supercomunidad"
        self.detectCommunities(cd_algorithm, gC_path, comm_label=comm_label2)

        # Dictionary projs: comms
        gP = self.SG.snodes[gP_label]  # Just an abbreviation
        p2c = dict(zip(gP.df_nodes['Id'], gP.df_nodes['Comunidad']))

        # Compute keywords for each commmunity
        W, com_keywords = self._get_community_labels(
            path2tm, p2c, n_keywords=n_keywords)

        # Add keywords to community snode
        self.SG.add_snode_attributes(gC_label, 'Claves', com_keywords)

        # ###############################
        # Graph Organizations-Communities

        logging.info('========== 4. Computing bigraph PROJECTS->COMMUNITIES')
        bgOC_path = os.path.join(path2bigraphs, bgOC_label)
        self.SG.transitive_graph(bgOC_label, bgPO_label, bgPC_label)

        # #############
        # Visualization
        logging.info('========== 5. Visualizing bipartite graph')

        # Attributes to visualize
        s_att1 = 'ORGANISMO OBJ SOCIAL'
        s_att2 = 'NSize'
        s_att2 = 'ORGANISMO OBJ SOCIAL'
        # s_att2 = 'TIPO ENTIDAD'
        t_att = comm_label2
        t_att2 = 'Claves'

        gO = self.SG.snodes[gO_label]   # Just an abbreviation
        if s_att2 == s_att1:
            s_att2 = "Organismo"
            att_values = gO.df_nodes[s_att1].tolist()
            self.SG.add_snode_attributes(gO_label, s_att2, att_values)

        if 'TIPO ENTIDAD' in gO.df_nodes:
            gO.df_nodes['TIPO ENTIDAD'] = gO.df_nodes['TIPO ENTIDAD'].apply(
                lambda x: '00' if x == 'ND' else x)
        # if 'Organismo' in gO.df_nodes:
        #     orgs = gO.df_nodes[s_att2].tolist()
        #     orgs = list(set(orgs))
        #     o2i = {o: f"KK{i}K,K" for i, o in enumerate(orgs)}
        #     gO.df_nodes[s_att2] = gO.df_nodes[s_att2].apply(
        #         lambda x: str(o2i[x]))

        th = 10
        self.SG.filter_edges_from_sedge(bgOC_label, th)

        # Save graph: nodes and edges. It is necessary to do this before the
        # call to display_bigraph to make sure that the graph folders exist.
        self.SG.save_supergraph()
        # Launch the visualization
        self.display_bigraph(bgOC_path, s_att1, s_att2, t_att, t_att2=t_att2,
                             template_html="demo_template.html",
                             template_js="demo_template.js")
        breakpoint()

        # Restore activation status
        self.keep_active = keep_active_old
        self.SG.keep_active = SG_keep_active_old

        return



